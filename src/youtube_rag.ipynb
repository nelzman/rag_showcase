{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube RAG\n",
    "\n",
    "You want to know what a youtube-video is about without spending the time to watch it?  \n",
    "You want to summarize what was told in a youtube-video?   \n",
    "You want to get information from an influencer via their youtube-videos without watching all of them?\n",
    "\n",
    "Then this might be an interesting notebook for you. \n",
    "Here I show you how to use youtube videos in combination with a RAG-Model to give you details about the video.\n",
    "RAG introduces an information retrieval component that uses user input to initially retrieve information from a new data source called the vectorstore in order to optimize performance and prize for the llm-calls.\n",
    "\n",
    "This notebook shows a straight forward approach for creating a RAG-Model with openAI (python package Version 1+) and langchain. You can also connect Langsmith with it so you can also track your calls to the RAG-Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import Runnable, RunnablePassthrough\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import openai\n",
    "from pytube import YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Usages\n",
    "### What are LLMs?\n",
    "\n",
    "Large Language Models (LLMs) are advanced artificial intelligence models capable of understanding and generating human-like text.  \n",
    "They are trained on vast amounts of data to learn the patterns and structures of language.  \n",
    "LLMs utilize deep learning techniques to process input text and generate coherent responses, enabling them to perform a wide range of natural language understanding and generation tasks.\n",
    "\n",
    "### What can LLMs do?\n",
    "\n",
    "#### Read\n",
    "You can give it a text and it can summarize it or categorize it (p.E. sentiment):  \n",
    "![GPT can read](../artifacts/images/youtube_rag/gpt_reading.png)\n",
    "\n",
    "This enables quite a view use-cases like categorizing reviews, translating text or summarizing large amounts of text.\n",
    "\n",
    "#### Write\n",
    "You can ask it to create text:  \n",
    "![GPT can write](../artifacts/images/youtube_rag/gpt_writing.png)\n",
    "\n",
    "This enables use-cases like content creation, brain-storming for ideas or \n",
    "#### Chat\n",
    "\n",
    "You can give the llm a knowledge of the previous conversation (called context) so it can reference previous interactions.  \n",
    "![GPT can chat](../artifacts/images/youtube_rag/gpt_chatting.png) \n",
    "\n",
    "Use-Cases are of course Chat-bots/Customer Support but also Q&As for eductation or training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Setup\n",
    "\n",
    "First of all we specify some paths to save the youtube videos and audios to.  \n",
    "Then we need to specify the api-keys for open-ai (necessary) and langsmith (not necessary) if accessable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_path = \"../artifacts/audio\"\n",
    "video_path = \"../artifacts/video\"\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"True\"\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = getpass.getpass('LangChain API Key:')\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"test_project\"\n",
    "\n",
    "client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Video(s)\n",
    "\n",
    "Here we specify the youtube videos for our knowledge-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.youtube.com/watch?v=nYdE3DDtNgA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Audio\n",
    "\n",
    "At first we need to extract the audio from the youtube videos and save them in `audio_path`.\n",
    "We can do that directly via the package `pytube` and in order to save discspace, for this use-case, we just download the audio-data and ignore the video itself.\n",
    "\n",
    "The next step is to transcribe the audio into text which is done with the `whisper-1` model from the `openai`-package.\n",
    "You can also use the `speechrecognition` package here.\n",
    "\n",
    "In order to be processed correctly and add some metadata there is a handy class from `langchain` calles `Document` to process the docs correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_from_youtube(url: str = 'https://youtu.be/2lAe1cqCOXo', audio_path: str = \"../artifacts/audio\") -> str:\n",
    "    \"\"\"\n",
    "    :param url: string url\n",
    "    :param audio_path: string audio_path\n",
    "    \n",
    "    - download audio from youtube\n",
    "    \"\"\"\n",
    "    os.makedirs(audio_path, exist_ok=True)\n",
    "    audio = YouTube(url).streams.filter(only_audio=True).first()\n",
    "    audio.download(output_path=audio_path)\n",
    "    return audio.default_filename\n",
    "\n",
    "def speech_to_text(client: object, audio_file_name: str = \"speech_recording.mp4\", \n",
    "                   file_path: str = \"../artifacts/audio\", language: str = \"en\") -> str:\n",
    "    \"\"\"\n",
    "    :param audio_file_name: string audio_file_name\n",
    "    :param language: string language\n",
    "    :return: string\n",
    "\n",
    "    - get a response from chatGPT\n",
    "    - convert speech to text\n",
    "    \"\"\"\n",
    "    with open(f\"{file_path}/{audio_file_name}\", \"rb\") as audio_file:\n",
    "        response = client.audio.transcriptions.create(model=\"whisper-1\", language=language, file=audio_file)\n",
    "        gpt_result = response.text\n",
    "\n",
    "    return gpt_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio file: Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "docs = []\n",
    "for url in urls:\n",
    "    audio_file = get_audio_from_youtube(url=url, audio_path=audio_path)\n",
    "    print(f\"audio file: {audio_file}\")\n",
    "    text = speech_to_text(client=client, audio_file_name=audio_file, file_path=audio_path, language=\"en\") \n",
    "    doc = Document(\n",
    "                page_content=text,\n",
    "                metadata={\"source\": audio_file, \"page\": 1},\n",
    "            )\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Documents\n",
    "\n",
    "Now we need to split the texts into context-chunks in order to give the rag enough context to make reasonable decisions but also keep the context small enough so that the call isn't too big and expensive.\n",
    "\n",
    "Chunk-size = 1000 and chunk_overlap = 200 means that each chunk has 1000 tokens (rule of thumb: 3 words refer to 4 tokens) with with an overlap of 200.\n",
    "\n",
    "Example: tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integer tokens: [3923, 596, 279, 6811, 1990, 11460, 323, 4339, 304, 3544, 4221, 4211, 1093, 6369, 2427, 418, 477, 423, 8788, 477, 445, 81101, 18, 30]\n",
      "String tokens: [b'What', b\"'s\", b' the', b' difference', b' between', b' tokens', b' and', b' words', b' in', b' large', b' language', b' models', b' like', b' chat', b'-g', b'pt', b' or', b' D', b'olly', b' or', b' L', b'lama', b'3', b'?']\n"
     ]
    }
   ],
   "source": [
    "text = \"What's the difference between tokens and words in large language models like chat-gpt or Dolly or Llama3?\"\n",
    "\n",
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "token_int = encoding.encode(text)\n",
    "print(f\"integer tokens: {token_int}\")\n",
    "token_str = [encoding.decode_single_token_bytes(token) for token in token_int]\n",
    "print(f\"String tokens: {token_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: chunk_size and chunk_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"What's the difference between tokens and\", 'and words in large language models like', 'like chat-gpt or Dolly or Llama3?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=40, chunk_overlap=10)\n",
    "splits = text_splitter.split_text(text)\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary code for the RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(docs: list, chunk_size: int = 1000, chunk_overlap: int = 200) -> list:\n",
    "    \"\"\"\n",
    "    :param docs: list of documents\n",
    "    :param chunk_size: integer chunk_size of the documents\n",
    "    :param chunk_overlap: integer chunk_overlap for each chunk\n",
    "    :return: list of splits\n",
    "\n",
    "    - split documents into chunks\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4', 'page': 1}\n",
      "996\n",
      "{'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4', 'page': 1}\n",
      "999\n",
      "{'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4', 'page': 1}\n",
      "998\n",
      "{'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4', 'page': 1}\n",
      "998\n",
      "{'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4', 'page': 1}\n",
      "999\n",
      "{'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4', 'page': 1}\n",
      "998\n",
      "{'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4', 'page': 1}\n",
      "998\n",
      "{'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4', 'page': 1}\n",
      "997\n",
      "{'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4', 'page': 1}\n",
      "996\n",
      "{'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4', 'page': 1}\n",
      "861\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splits = split_docs(docs, chunk_size=1000, chunk_overlap=200)\n",
    "for doc in splits:\n",
    "    print(doc.metadata)\n",
    "    print(len(doc.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vectorstore\n",
    "\n",
    "Now we have quite some context-docs to enable our llm to answer questions about the video. But in order to give the right context out of all these context-docs to the llm we need to create something that's called a Vectorstore. \n",
    "\n",
    "Vectorstores basically transform the context into numerical vectors where you then make an comparison of the question (also transformed to a vector) and the context. And the context that fits best to the question is given to the llm for reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(\n",
    "    splits: list,\n",
    "    vectore_store_name: str = \"chromadb\",\n",
    ") -> object:  \n",
    "    \"\"\"\n",
    "    :param splits: list[Document] splits to create the vectorstore\n",
    "    :param vectore_store_name: string vectore_store_name to choose the vectorstore\n",
    "    :return: object\n",
    "\n",
    "    - create a vectorstore from the pages\n",
    "    - different vectorstore options are available\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    if vectore_store_name == \"chromadb\":\n",
    "        from langchain_community.vectorstores import Chroma\n",
    "        vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "    elif vectore_store_name == \"deeplake\":\n",
    "        from langchain_community.vectorstores import DeepLake\n",
    "        vectorstore = DeepLake.from_documents(\n",
    "            documents=splits,\n",
    "            dataset_path=\"./my_deeplake/\",\n",
    "            embedding=embeddings,\n",
    "            overwrite=True,\n",
    "        )\n",
    "\n",
    "    elif vectore_store_name == \"faiss\":\n",
    "        from langchain_community.vectorstores import FAISS\n",
    "        vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "    elif vectore_store_name == \"annoy\":\n",
    "        from langchain_community.vectorstores import Annoy\n",
    "        vectorstore = Annoy.from_documents(splits, embeddings)\n",
    "\n",
    "    elif vectore_store_name == \"docarray_hnsw_search\":\n",
    "        from langchain.vectorstores.docarray.hnsw import DocArrayHnswSearch\n",
    "        vectorstore = DocArrayHnswSearch.from_documents(splits, embeddings, work_dir=\"hnswlib_store/\", n_dim=1536)\n",
    "\n",
    "    elif vectore_store_name == \"docarray_inmemory\":\n",
    "        from langchain_community.vectorstores.docarray import DocArrayInMemorySearch\n",
    "        vectorstore = DocArrayInMemorySearch.from_documents(splits, embeddings)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown vectorstore name\")\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorstore = create_vectorstore(splits, vectore_store_name=\"chromadb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Retreiver\n",
    "\n",
    "The retreiver is a lightweight wrapper around the vectorstore that lets you query questions and retrieve the contents that are closest to the question.\n",
    "\n",
    "By default it uses similarity-search for finding the best fitting answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"outfit here was amongst my favorite looks of his. Just a gray jacket, faded olive green inside, and a burgundy base tie. So what'd you think? If you watched this film, did you previously notice the things I mentioned? Most of my observations came from watching this film for a second time, which is just one of the things I love about cinema. It's okay if everyone doesn't see it or get it, it's meant to be seen again and again, and with every watch, there is always something new to discover. If you want to say in what movies I get to review next, I have polls on my Twitter twice a week, and if I somehow get a chance to watch any movies outside of the ones I review, you can follow my letterboxd to keep updated on that. Thank you so so much to the American Film Institute for providing the 100 Greatest American Films of All Time list. One down, 99 to go.\", metadata={'page': 1, 'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4'}),\n",
       " Document(page_content=\"outfit here was amongst my favorite looks of his. Just a gray jacket, faded olive green inside, and a burgundy base tie. So what'd you think? If you watched this film, did you previously notice the things I mentioned? Most of my observations came from watching this film for a second time, which is just one of the things I love about cinema. It's okay if everyone doesn't see it or get it, it's meant to be seen again and again, and with every watch, there is always something new to discover. If you want to say in what movies I get to review next, I have polls on my Twitter twice a week, and if I somehow get a chance to watch any movies outside of the ones I review, you can follow my letterboxd to keep updated on that. Thank you so so much to the American Film Institute for providing the 100 Greatest American Films of All Time list. One down, 99 to go.\", metadata={'page': 1, 'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4'}),\n",
       " Document(page_content=\"without Francis Ford Coppola's amazing direction, and just Francis Ford Coppola in general, so I'm still a little bit salty about his Oscar snub. So my impression of the film? Uh, I loved it. I envy Italian families, they're so big and loving and fun, just look how joyous this wedding scene is. This scene takes place right at the beginning of the film, while simultaneously cutting between business meetings. At first I thought the wedding scene was overexposed, until you realize how underexposed the office scenes are, even with the windows open to broad daylight. This is the first shot of the movie, and it's one take that lasts for three minutes, ending with these two characters' silhouette. I view this choice as two guys participating in shady business, hence their blackout. Aside from that and overall, I was just in awe of every single location they shot at. So in pre-production for a film, there is a location team made up of scouts and managers who determine what location would work\", metadata={'page': 1, 'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4'}),\n",
       " Document(page_content=\"without Francis Ford Coppola's amazing direction, and just Francis Ford Coppola in general, so I'm still a little bit salty about his Oscar snub. So my impression of the film? Uh, I loved it. I envy Italian families, they're so big and loving and fun, just look how joyous this wedding scene is. This scene takes place right at the beginning of the film, while simultaneously cutting between business meetings. At first I thought the wedding scene was overexposed, until you realize how underexposed the office scenes are, even with the windows open to broad daylight. This is the first shot of the movie, and it's one take that lasts for three minutes, ending with these two characters' silhouette. I view this choice as two guys participating in shady business, hence their blackout. Aside from that and overall, I was just in awe of every single location they shot at. So in pre-production for a film, there is a location team made up of scouts and managers who determine what location would work\", metadata={'page': 1, 'source': 'Why â€˜The Godfather is the GREATEST MOVIE OF ALL TIME.mp4'})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever.invoke(\"Why does the film appeal to the reviewer?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Prompt\n",
    "\n",
    "The langchain-community-hub provides different prompts for different use-cases. You can use p.E. `rlm/rag-prompt` which is a quite powerful prompt for knowledge extraction without halluzination.\n",
    "\n",
    "We can also use a slightly modified version from the hub I created, where a \"don't know\" answer starts with \"Sorry\" and thus can be caught and handled: `nelzman/rag-prompt-with-dunno-catch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know and start with 'Sorry' continued in the language of the question. Use three sentences maximum, just answer the question and keep the answer concise. The language of the Answer should be the same as the question.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "#prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt = hub.pull(\"nelzman/rag-prompt-with-dunno-catch\")\n",
    "\n",
    "print(prompt.to_json()[\"kwargs\"][\"template\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Large Language Model\n",
    "\n",
    "Here we are using `gpt-3.5-turbo-1106` from openAI to create the llm-interface for the RAG.  \n",
    "One important parameter is the `temperature` which is used to control the randomness of the outputs.  \n",
    "Since we are creating a rag to retreive accurate information, we are setting `temperature == 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-1106\",  # currently only works with gpt-3.5-turbo-1106\n",
    "    temperature=0, # less creative responses\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RAG-Chain\n",
    "\n",
    "Langchain is a Interface that enables chaining different parts of a execution-pipeline together into one object and thus enables a robust way to interact and use the RAG-model.\n",
    "\n",
    "First we send the question to the retriever which gives us the best fitting context.  \n",
    "Then the Docs get formatted and giiven to the prompt.  \n",
    "The prompt then gets sent to the llm which gives back a json-like object which then gets parsed to string.  \n",
    "\n",
    "Et voilÃ , we have the anwer to our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list) -> str:\n",
    "    \"\"\"\n",
    "    :param docs: list of documents to format\n",
    "    :return: string\n",
    "\n",
    "    - format the documents\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def create_rag_chain(retriever: object, prompt: object, llm: object) -> object:\n",
    "    \"\"\"\n",
    "    :param retriever: vectorstore retreiver for the answer\n",
    "    :param prompt: prompt for the question\n",
    "    :param llm: large language model for the answer\n",
    "\n",
    "    \n",
    "    - create chain: retriever -> format_docs -> prompt -> llm -> StrOutputParser\n",
    "    \"\"\"\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_chain = create_rag_chain(retriever, prompt, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Test\n",
    "\n",
    "This is just a Test of the things we implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reviewer loved the film because of the amazing direction by Francis Ford Coppola and the portrayal of Italian families. They also appreciated the cinematography and the choice of locations for the film. The reviewer also mentioned that they enjoyed watching the film for a second time and discovering new things with each viewing.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Why does the film appeal to the reviewer?\"\n",
    "response1 = rag_chain.invoke(question)\n",
    "print(response1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use a callback function to get the response from the openai model. `tiktoken` package is needed for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rag_chain(rag_chain: object, prompt: str = None):\n",
    "    with get_openai_callback() as cb:\n",
    "        response = rag_chain.invoke(prompt)\n",
    "    return response, cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reviewer loved the film because of the amazing direction by Francis Ford Coppola and the portrayal of joyous Italian family scenes. They also appreciated the cinematography and the choice of locations for the film. The reviewer also mentioned being in awe of the wedding scene and the overall impression of the movie.\n",
      "Tokens Used: 976\n",
      "\tPrompt Tokens: 917\n",
      "\tCompletion Tokens: 59\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0010350000000000001\n"
     ]
    }
   ],
   "source": [
    "response2, rag_callback = check_rag_chain(rag_chain=rag_chain, prompt=question)\n",
    "print(response2)\n",
    "print(rag_callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catching Dunno\n",
    "\n",
    "If you don't find an answer, you can also create paraphrases for your question and then loop through them until you find an answer to your question.  \n",
    "Since we are using the prompt that always starts with \"Sorry\" if it didn't find an answer, we can loop through the paraphrases until it finds an answer to a \"close enough\" question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What makes the movie attractive to the critic? ', ' What is it about the film that captivates the reviewer? ', ' Why does the reviewer find the film appealing? ', \" What is the reason behind the reviewer's attraction to the film? \", ' What is it about the movie that draws the reviewer in?']\n"
     ]
    }
   ],
   "source": [
    "paraphrases = llm.invoke(f\"Create 5 paraphrases of the question: {question} Seperate them by |||\")\n",
    "paraphrases_list = [value.replace(\"\\n\", \"\") for value in paraphrases.content.split(\"|||\")]\n",
    "print(paraphrases_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First question to retreive an answer: What makes the movie attractive to the critic? \n",
      "Answer: The critic found the movie attractive due to Francis Ford Coppola's amazing direction and the portrayal of Italian family life. They also appreciated the cinematography, particularly the use of lighting and the choice of filming locations. The film's ability to be rewatched and discover new details with each viewing also added to its appeal for the critic.\n"
     ]
    }
   ],
   "source": [
    "for question in paraphrases_list:\n",
    "    response3 = rag_chain.invoke(question)\n",
    "    if not response3.startswith(\"Sorry\"):\n",
    "        break\n",
    "print(f\"First question to retreive an answer: {question}\")\n",
    "print(f\"Answer: {response3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
